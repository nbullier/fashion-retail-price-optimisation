{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "\n",
    "from sklearn.metrics import (r2_score, mean_squared_error, median_absolute_error, \n",
    "mean_absolute_error, mean_absolute_percentage_error)\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import RandomizedSearchCV\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Code for the TimeBasedCV class adapted from:\n",
    "https://towardsdatascience.com/time-based-cross-validation-d259b13d42b8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import datetime\n",
    "from datetime import datetime as dt\n",
    "from dateutil.relativedelta import *\n",
    "\n",
    "\n",
    "class TimeBasedCV(object):\n",
    "    '''\n",
    "    Adpated from https://towardsdatascience.com/time-based-cross-validation-d259b13d42b8\n",
    "    \n",
    "    Parameters \n",
    "    ----------\n",
    "    train_period: int\n",
    "        number of time units to include in each train set\n",
    "        default is 30 # TO FIX - this has been changed\n",
    "    test_period: int\n",
    "        number of time units to include in each test set \n",
    "        default is 7\n",
    "    freq: string\n",
    "        frequency of input parameters. possible values are: days, months, years, weeks, hours, minutes, seconds\n",
    "        possible values designed to be used by dateutil.relativedelta class\n",
    "        default is weeks\n",
    "    '''\n",
    "    \n",
    "    \n",
    "    def __init__(self, train_period=30, test_period=7, freq='weeks'):\n",
    "        self.train_period = train_period\n",
    "        self.test_period = test_period\n",
    "        self.freq = freq\n",
    "\n",
    "        \n",
    "        \n",
    "    def split(self, data, validation_split_date=None, date_column='transaction_date', gap=0):\n",
    "        '''\n",
    "        Generate indices to split data into training and test set\n",
    "        \n",
    "        Parameters \n",
    "        ----------\n",
    "        data: pandas DataFrame\n",
    "            your data, contain one column for the record date \n",
    "        validation_split_date: datetime.date()\n",
    "            first date to perform the splitting on.\n",
    "            if not provided will set to be the minimum date in the data after the first training set\n",
    "        date_column: string, deafault='transaction_date'\n",
    "            date of each record\n",
    "        gap: int, default=0\n",
    "            for cases the test set does not come right after the train set,\n",
    "            *gap* days are left between train and test sets\n",
    "        \n",
    "        Returns \n",
    "        -------\n",
    "        train_index ,test_index: \n",
    "            list of tuples (train index, test index) similar to sklearn model selection\n",
    "        '''\n",
    "        \n",
    "        # check that date_column exist in the data:\n",
    "        try:\n",
    "            data[date_column]\n",
    "        except:\n",
    "            raise KeyError(date_column)\n",
    "                    \n",
    "        train_indices_list = []\n",
    "        test_indices_list = []\n",
    "\n",
    "        if validation_split_date==None:\n",
    "            validation_split_date = data[date_column].min().date() + eval('relativedelta('+self.freq+'=self.train_period)')\n",
    "        \n",
    "        # initalise start/end of train/test periods\n",
    "        start_train = data[date_column].min().date()\n",
    "        end_train = validation_split_date \n",
    "        start_test = validation_split_date + datetime.timedelta(days=1)\n",
    "        end_test = start_test + eval('relativedelta('+self.freq+'=self.test_period)')\n",
    "\n",
    "        count = 1\n",
    "\n",
    "        while end_test < (data[date_column].max().date() + datetime.timedelta(days=1)):\n",
    "            # train indices:\n",
    "            cur_train_indices = list(data[(data[date_column].dt.date>=start_train) & \n",
    "                                     (data[date_column].dt.date<end_train)].index)\n",
    "\n",
    "            # test indices:\n",
    "            cur_test_indices = list(data[(data[date_column].dt.date>=start_test) &\n",
    "                                    (data[date_column].dt.date<end_test)].index)\n",
    "            \n",
    "            print(\"Train period:\",start_train,\"-\" , end_train, \", Test period\", start_test, \"-\", end_test,\n",
    "                  \"# train records\", len(cur_train_indices), \", # test records\", len(cur_test_indices))\n",
    "\n",
    "            train_indices_list.append(cur_train_indices)\n",
    "            test_indices_list.append(cur_test_indices)\n",
    "\n",
    "            count += 1\n",
    "\n",
    "            # update dates:\n",
    "            start_train = data[date_column].min().date()             \n",
    "            start_test = start_test + datetime.timedelta(weeks=9)\n",
    "            end_train = start_test - datetime.timedelta(days=1)\n",
    "            if count == 3:\n",
    "                end_test = data[date_column].max().date() \n",
    "            else:\n",
    "                end_test = start_test + datetime.timedelta(weeks=8)\n",
    "\n",
    "        # mimic sklearn output  \n",
    "        index_output = [(train,test) for train,test in zip(train_indices_list,test_indices_list)]\n",
    "\n",
    "        self.n_splits = len(index_output)\n",
    "        \n",
    "        return index_output\n",
    "    \n",
    "    \n",
    "    def get_n_splits(self):\n",
    "        \"\"\"Returns the number of splitting iterations in the cross-validator\n",
    "        Returns\n",
    "        -------\n",
    "        n_splits : int\n",
    "            Returns the number of splitting iterations in the cross-validator.\n",
    "        \"\"\"\n",
    "        return self.n_splits "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "sales = pd.read_csv(\"../data/interim/transactions_sd_knits_resampled_engin_synth_gt_gb.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data(df):\n",
    "    '''\n",
    "    Returns appropriate data type and drops columns irrelavent to modelling process\n",
    "\n",
    "        Parameters:\n",
    "            df (dataframe): transactions dataframe with engineered features\n",
    "\n",
    "        Returns:\n",
    "            df (dataframe): transformed transactions dataframe with engineered features\n",
    "    '''\n",
    "    df['transaction_date'] = pd.to_datetime(df['transaction_date'], infer_datetime_format=True)\n",
    "    df.sort_values(by=['transaction_date'])\n",
    "    df['week_no'] = df['week_no'].astype('object')\n",
    "    df['review'] = df['review'].astype('object')\n",
    "    df.drop(columns=['month'], inplace=True)\n",
    "    df.drop(columns=['p_id'], inplace=True)\n",
    "    return(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot_encode_categorical(df):\n",
    "    '''\n",
    "    Returns dataframe with all object datatypes in dataframe one hot encoded. Drops \n",
    "    specific columns to get k-1 dummies.\n",
    "        Parameters:\n",
    "            df (dataframe): transactions dataframe with engineered features\n",
    "        Returns:\n",
    "            df_encoded (dataframe): transformed input\n",
    "    '''\n",
    "\n",
    "    df_encoded = pd.get_dummies(df) \n",
    "    df_encoded.drop(columns=['week_no_2', 'label_desc_lab_1', 'color_simple_Other', \n",
    "                            'review_0.0'], \n",
    "                    axis=1, \n",
    "                    inplace=True)\n",
    "    return df_encoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_price_quantity(df):\n",
    "    '''\n",
    "    Returns log values of price and quantity. \n",
    "    \n",
    "            Parameters:\n",
    "                df (dataframe): transactions dataframe\n",
    "\n",
    "            Returns:\n",
    "                df (dataframe): transformed input\n",
    "    '''\n",
    "    \n",
    "    df['price_log'] = np.log(df['price'] + 1) # +1 to avoid senario of log(0)\n",
    "    df['quantity_log'] = np.log(df['quantity'] + 1) # +1 to avoid senario of log(0) \n",
    "    df.drop(columns=['price'], inplace=True)\n",
    "    df.drop(columns=['quantity'], inplace=True)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def temporal_test_train_split_for_cv(df, split_date='2021-10-3'):\n",
    "   '''\n",
    "   Function splits dataframe based on a specific date test X and y datasets ready for\n",
    "   model prediction and returns unaltered dataframe of training data. Allows\n",
    "   training data to be processed for CV splits by apply_tscv_split_to_training \n",
    "\n",
    "      Parameters:\n",
    "            df (dataframe): transactions dataframe\n",
    "            split_date (str): date in 'YYYY-MM-DD' (default '2021-10-3')\n",
    "\n",
    "      Returns:\n",
    "            df_train (dataframe): dataframe of training data\n",
    "            X_test (dataframe): X test set (model test inputs)\n",
    "            y_test_log (dataframe): y test set (model test targets) \n",
    "   '''\n",
    "\n",
    "   df_train = df[df['transaction_date'] < split_date]\n",
    "   df_test = df[df['transaction_date'] >= split_date]\n",
    "\n",
    "   y_test_log = df_test['quantity_log']\n",
    "   X_test = df_test.drop(['quantity_log', 'transaction_date'], axis=1)\n",
    " \n",
    "   return df_train, X_test, y_test_log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_tscv_split_to_training(df):\n",
    "    '''\n",
    "    Calls function to generate indicies for time series splitting of the training data which \n",
    "    incrementally increases the size of the training set while pushing along the validation set. \n",
    "    Currently set to create 3 folds. To be applied to the df_train dataframe already processed through \n",
    "    temporal_test_train_split_for_cv. NB: could be refined to have less hard code.\n",
    "    \n",
    "        Parameters:\n",
    "            df (dataframe): df_train\n",
    "        Returns:\n",
    "            index_output (list of tuples): train_index, test_index list similar to sklearn model selection\n",
    "            X_train (dataframe): X train set (model train inputs)\n",
    "            y_train_log (dataframe): y train set (model train targets)\n",
    "\n",
    "    '''\n",
    "    \n",
    "    tscv = TimeBasedCV(train_period=20,\n",
    "                        test_period=7,\n",
    "                        freq='weeks')\n",
    "        \n",
    "    index_output = tscv.split(df, validation_split_date=datetime.date(2021,4,10), date_column='transaction_date')\n",
    "\n",
    "    train_labels_log = df['quantity_log']\n",
    "    train_features = df.drop(['quantity_log', 'transaction_date'], axis=1)\n",
    "    \n",
    "    return index_output, train_features, train_labels_log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_results_dataframes(): \n",
    "    '''\n",
    "    Returns dataframes to store metrics 'R2', 'RMSE', 'MAE', 'MAPE', 'MedAE','MedAPE'\n",
    "    from training and testing data. To be used with print_model_metrics.\n",
    "\n",
    "            Parameters:\n",
    "\n",
    "            Returns:\n",
    "                train_results_df (dataframe): empty dataframe for populating\n",
    "                test_results_df (dataframe): empty dataframe for populating\n",
    "    '''\n",
    "        \n",
    "    train_results_df = pd.DataFrame(index=['R2', 'RMSE', 'MAE', 'MAPE', 'MedAE','MedAPE'])\n",
    "    test_results_df = pd.DataFrame(index=['R2', 'RMSE', 'MAE', 'MAPE', 'MedAE', 'MedAPE'])\n",
    "    return train_results_df, test_results_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to output suite of metrics to dataframes\n",
    "def print_model_metrics (model, y_train_log, X_train, y_test_log, X_test, \n",
    "                        model_name, train_results_df, test_results_df):\n",
    "    '''\n",
    "    Calculates 'R2', 'RMSE', 'MAE', 'MAPE', 'MedAE','MedAPE' for training and test data given \n",
    "    inputed model and stores in provided dataframe.  \n",
    "\n",
    "        Parameters:\n",
    "            model (estimator): model to run data through\n",
    "            y_train_log (*array): y for training data\n",
    "            X_train (*array): X for training data\n",
    "            y_test_log (*array): y for test data\n",
    "            X_test (*array): X for test data\n",
    "            model_name (str): becomes name of column in results dataframes\n",
    "            train_results_df (dataframe): dataframe to store model metrics\n",
    "            test_results_df (dataframe): dataframe to store model metrics\n",
    "        Returns:\n",
    "        '''\n",
    "\n",
    "    print(model_name + ' model:'), \n",
    "    y_train = np.exp(y_train_log)-1\n",
    "    y_test = np.exp(y_test_log)-1\n",
    "    y_pred_train = np.exp(model.predict(X_train))-1\n",
    "    y_pred_test = np.exp(model.predict(X_test))-1\n",
    "\n",
    "    r2_train = round(r2_score(y_train, y_pred_train),3)\n",
    "    rmse_train = round(mean_squared_error(y_train, y_pred_train)**0.5,3)\n",
    "    mae_train = round(mean_absolute_error(y_train, y_pred_train), 3)\n",
    "    mape_train = round(mean_absolute_percentage_error(y_train, y_pred_train)*100,3)\n",
    "    medae_train = round(median_absolute_error(y_train, y_pred_train),3)\n",
    "    medAPE_train = round(np.median(np.abs((y_train - y_pred_train)/y_train))*100, 3)\n",
    "\n",
    "    r2_test = round(r2_score(y_test, y_pred_test),3)\n",
    "    rmse_test = round(mean_squared_error(y_test, y_pred_test)**0.5,3)\n",
    "    mae_test = round(mean_absolute_error(y_test, y_pred_test), 3)\n",
    "    mape_test = round(mean_absolute_percentage_error(y_test, y_pred_test)*100,3)\n",
    "    medae_test = round(median_absolute_error(y_test, y_pred_test),3)\n",
    "    medAPE_test = round(np.median(np.abs((y_test - y_pred_test)/y_test))*100, 3)\n",
    "\n",
    "    train_results_df[model_name]=[r2_train, rmse_train, mae_train, mape_train, medae_train, medAPE_train]\n",
    "    test_results_df[model_name]=[r2_test, rmse_test, mae_test, mape_test, medae_test, medAPE_test]\n",
    "\n",
    "    display(train_results_df)\n",
    "    display(test_results_df)\n",
    "\n",
    "    #return train_results_df, test_results_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knit_data = prepare_data(sales)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knit_data = one_hot_encode_categorical(knit_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knit_data = log_price_quantity(knit_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train, X_test, y_test_log = temporal_test_train_split_for_cv(knit_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_output, X_train, y_train_log = apply_tscv_split_to_training(df_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply timeseries cv split to the Random Forest regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_best_random_forest_hyperparameters(X_train, y_train_log, index_output):\n",
    "    '''\n",
    "    Returns best random forest estimator based on random search of 100 interations through parameter grid\n",
    "    which is included in the function.\n",
    "        Parameters:\n",
    "            X_train (*array): X for training data\n",
    "            y_train_log (*array): y for training data\n",
    "            index_output (list of tuples): train_index, test_index list similar to sklearn model selection\n",
    "        Returns:\n",
    "            rf_tuned_model (estimator): best estimator from the random search CV process \n",
    "    '''\n",
    "    \n",
    "    # Set up parameters for optimisation\n",
    "\n",
    "    # Number of trees in random forest\n",
    "    n_estimators = [int(x) for x in np.linspace(start = 100, stop = 1000, num = 6)]\n",
    "    # Criterion to measure quality of a split\n",
    "    criterion = ['squared_error', 'absolute_error', 'poisson']\n",
    "    # Number of features to consider at every split\n",
    "    max_features = list(range(2, len(X_train.columns), 2))\n",
    "    # Maximum number of levels in tree\n",
    "    max_depth = list(range(2,10))\n",
    "    # Minimum number of samples required to split a node\n",
    "    min_samples_split = [2, 5, 10]\n",
    "    # Minimum number of samples required at each leaf node\n",
    "    min_samples_leaf = [1, 2, 4]\n",
    "    # Method of selecting samples for training each tree\n",
    "    bootstrap = [True, False]\n",
    "\n",
    "    # Create the random grid\n",
    "    random_grid = {'n_estimators': n_estimators,\n",
    "                    'criterion': criterion,\n",
    "                    'max_features': max_features,\n",
    "                    'max_depth': max_depth,\n",
    "                    'min_samples_split': min_samples_split,\n",
    "                    'min_samples_leaf': min_samples_leaf,\n",
    "                    'bootstrap': bootstrap}\n",
    "\n",
    "    print(random_grid)\n",
    "\n",
    "    # Use the random grid to search for best hyperparameters\n",
    "\n",
    "    # First create the base model to tune\n",
    "    rf = RandomForestRegressor(random_state=0)\n",
    "    # Random search of parameters, using 3 fold cross validation, \n",
    "    # search across 100 different combinations, and use all available cores\n",
    "    rf_tuned_model = RandomizedSearchCV(estimator = rf, \n",
    "                                    param_distributions = random_grid, \n",
    "                                    n_iter = 100, \n",
    "                                    cv = index_output, # indexes generated from tscv\n",
    "                                    verbose=2, \n",
    "                                    random_state=42, \n",
    "                                    n_jobs = -1) \n",
    "    # Fit the random search model\n",
    "    rf_tuned_model.fit(X_train, y_train_log)\n",
    "\n",
    "    return rf_tuned_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the following to get result\n",
    "# rf_tuned_model = find_best_random_forest_hyperparameters(X_train, y_train_log, index_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#rf_tuned_model.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_results_df, test_results_df = create_results_dataframes()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Best Random Forest Model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_best_model = RandomForestRegressor(max_features=50,\n",
    "                                    max_depth=6,\n",
    "                                    n_estimators=820,\n",
    "                                    random_state=0,\n",
    "                                    min_samples_split=2,\n",
    "                                    min_samples_leaf=2,\n",
    "                                    criterion='squared_error',\n",
    "                                    bootstrap=False\n",
    "                                    ).fit(X_train, y_train_log)\n",
    "\n",
    "model_name = 'rf_best_model '\n",
    "print_model_metrics (rf_best_model , y_train_log, X_train, y_test_log, X_test, model_name, train_results_df, test_results_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Return weight of features in order of importance\n",
    "feature_names = X_train.columns\n",
    "coef = rf_best_model.feature_importances_\n",
    "# sort them out in descending order\n",
    "indices = np.argsort(abs(coef))[::-1]\n",
    "#print(indices)\n",
    "#print(feature_names[indices.astype(int)])\n",
    "\n",
    "for i in indices:\n",
    "    print(feature_names[i], ':', coef[i])"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "1045e0dd9445650e46d10e5fd886991556873f5daa36d5c504715a88762688b8"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
